{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape [(None, 10, 1), (None, 1)]\n",
      "input shape [(None, 10, 1), (None, 1)]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_i (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_j (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "base_vgg (Model)                (None, 10, 1)        20158282    input_i[0][0]                    \n",
      "                                                                 input_j[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_user (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "theta_layer (ThetaLayer)        (None, 1)            10000       base_vgg[1][0]                   \n",
      "                                                                 input_user[0][0]                 \n",
      "                                                                 base_vgg[2][0]                   \n",
      "                                                                 input_user[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "substract (Subtract)            (None, 1)            0           theta_layer[0][0]                \n",
      "                                                                 theta_layer[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Activation)            (None, 1)            0           substract[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,168,282\n",
      "Trainable params: 20,168,282\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_i (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "base_vgg (Model)                (None, 10, 1)        20158282    input_i[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_user (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "theta_layer (ThetaLayer)        (None, 1)            10000       base_vgg[1][0]                   \n",
      "                                                                 input_user[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           theta_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 20,168,282\n",
      "Trainable params: 20,168,282\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "w = 224\n",
    "h = 224\n",
    "input_shape = (w, h, 3)\n",
    "dropout = 0.2\n",
    "latent_d = 10 # latent dimension\n",
    "\n",
    "user_num = 1000 # for test, this should be obtained from the dataset\n",
    "\n",
    "\n",
    "def euclidean_distance(vects): \n",
    "    # L2 distance\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_i = layers.Input(shape=input_shape, name=\"input_i\") # image of item i\n",
    "input_j = layers.Input(shape=input_shape, name=\"input_j\") # image of item j\n",
    "input_idx = layers.Input(shape=[1], name=\"input_user\", dtype='int32') # idx of user u\n",
    "\n",
    "\n",
    "# customer layer, learn the latent matrix of theta_u\n",
    "class ThetaLayer(Layer):\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='theta_u_matrix', \n",
    "                                      shape=(user_num, latent_d),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(ThetaLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        assert isinstance(x, list)\n",
    "        item, u = x # u: user idx; item: visual feature of item \n",
    "        return K.dot(K.gather(self.kernel, u), item)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print('input shape', input_shape)\n",
    "        return (input_shape[0][0], 1)\n",
    "\n",
    "\n",
    "# load the VGG pretrained on imagenet\n",
    "def create_base_vgg(dropout):\n",
    "    vgg = keras.applications.vgg19.VGG19(\n",
    "        include_top=False, # whether to include the fc layers\n",
    "        weights='imagenet', \n",
    "        input_tensor=None, \n",
    "        input_shape=input_shape, \n",
    "        pooling='avg',  # in my experience, gloable avg works better than flatten, need to check\n",
    "        classes=1000)\n",
    "    x = vgg.output\n",
    "#     x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(256, activation='relu', name='fc1')(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(latent_d, activation='relu', name='predictions')(x)\n",
    "    x = layers.Reshape(target_shape=(latent_d, 1))(x)\n",
    "    \n",
    "    return Model(inputs = vgg.input, outputs = x, name=\"base_vgg\")\n",
    "\n",
    "\n",
    "\n",
    "# because we re-use the same instance `base_vgg`, theta_layer,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "base_vgg = create_base_vgg(dropout)\n",
    "theta = ThetaLayer(name='theta_layer')\n",
    "\n",
    "x1 = base_vgg(input_i)\n",
    "x1 = theta([x1, input_idx])\n",
    "\n",
    "x2 = base_vgg(input_j)\n",
    "x2 = theta([x2, input_idx])\n",
    "\n",
    "# distance = layers.Lambda(euclidean_distance,\n",
    "#                   output_shape=eucl_dist_output_shape)([x1, x2])\n",
    "            \n",
    "distance = layers.Subtract(name='substract')([x1, x2])\n",
    "distance = layers.Activation(activation='sigmoid', name='sigmoid')(distance)\n",
    "\n",
    "model = Model([input_i, input_j, input_idx], distance)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -----------Evaluation---------------\n",
    "# evaluation is different from training, \n",
    "# input of training: [user u, item i, item j]; \n",
    "# input of evaluation: [user u, item i]\n",
    "predict_score = layers.Activation(activation='sigmoid')(x1)\n",
    "evaluation_model = Model([input_i, input_idx], predict_score)\n",
    "evaluation_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f71733f2485e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mItem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musernum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musernum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "\n",
    "dataset_name = 'AmazonFashion6ImgPartitioned.npy'\n",
    "dataset_dir = '../dataset/amazon/'\n",
    "dataset = np.load(dataset_dir + dataset_name, encoding = 'latin1')\n",
    "\n",
    "[user_train, user_validation, user_test, Item, usernum, itemnum] = dataset\n",
    "\n",
    "print(user_train, user_validation, user_test, Item, usernum, itemnum)\n",
    "\n",
    "def load_image(file_path):\n",
    "    img = image.load_img(f_path, target_size=(w, h))\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------------------metrics---------------------------\n",
    "# auc_roc as the metrics\n",
    "# for details about how to implement auc in keras, see https://github.com/keras-team/keras/issues/3230\n",
    "def auc(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P\n",
    "\n",
    "\n",
    "# ----------------------loss function---------------------------\n",
    "# not sure which loss function is better\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    sqaure_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def entropy_loss(y_true, y_pred):\n",
    "    ''' from Comparative Deep Learning of Hybrid Representations for Image Recommendations\n",
    "    https://arxiv.org/pdf/1604.01252.pdf\n",
    "    use crose entropy as the loss\n",
    "    '''\n",
    "    margin = 1\n",
    "    sqaure_pred = -K.log(y_pred)\n",
    "    margin_square = -K.log(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "# ----------------------optimizer---------------------------\n",
    "# optimizer: rms or adam?\n",
    "rms = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "# first: freeze all convolutional layers, only train fc layers (which were randomly initialized)\n",
    "for layer in base_vgg.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "for layer in base_vgg.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# set trainable layers before model compile\n",
    "model.compile(optimizer=rms, loss=contrastive_loss)\n",
    "# model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "# evaluation_model.evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
    "\n",
    "# second: train the last conv block\n",
    "for layer in base_vgg.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "for layer in base_vgg.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# set trainable layers before model compile\n",
    "model.compile(optimizer=rms, loss=contrastive_loss)\n",
    "# model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "# evaluation_model.evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
